{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam Second Semester 2566 - Neural Network (Dry Bean Problem)\n",
    "\n",
    "This exam problem has an objective to develop a neural network model to classify the dry bean as belonging to one out of seven types (classes) from 16 features where\n",
    "\n",
    "type 0 = Seker,\n",
    "\n",
    "type 1 = Barbunya,\n",
    "\n",
    "type 2 = Bombay,\n",
    "\n",
    "type 3 = Cali,\n",
    "\n",
    "type 4 = Horoz,\n",
    "\n",
    "type 5 = Sira, and\n",
    "\n",
    "type 6 = Dermosan\n",
    "\n",
    "Cr: This dataset is adapted from KOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# library written for this exam\n",
    "import utilsNN as utils\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import random \n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start the exam by first loading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset and test dataset\n",
    "\n",
    "# Read tab separated data\n",
    "data = np.loadtxt(os.path.join('Data', 'NN_BeanData_Train.txt'))\n",
    "\n",
    "# First 16 columns of data are features and the last column is the label.\n",
    "# Matrix X contains 16 features while vector y contains the label.\n",
    "\n",
    "X, y = data[:, :16], data[:, 16].astype(int)\n",
    "\n",
    "m = y.size  # number of training examples\n",
    "\n",
    "# Read tab separated testing data\n",
    "data_test = np.loadtxt(os.path.join('Data', 'NN_BeanData_Test.txt'))\n",
    "\n",
    "X_test, y_test = data_test[:, :16], data_test[:, 16].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 16), (1500, 16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have been provided with a set of initialized network parameters ($\\Theta^{(1)}, \\Theta^{(2)}$). These are stored in `InitBeanWeight1.txt` and `InitBeanWeight2.txt` which will be loaded in the next cell of this notebook into `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 40 units in the second layer (hidden layer) and 7 output units (corresponding to 7 dry bean types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initiallized network parameters\n",
    "\n",
    "Theta1 = np.loadtxt(os.path.join('Data', 'InitBeanWeight1.txt'))\n",
    "Theta2 = np.loadtxt(os.path.join('Data', 'InitBeanWeight2.txt'))\n",
    "\n",
    "# Unroll parameters \n",
    "# To unroll the matrix into vector (1-D array), we use `np.ravel()` \n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "initial_nn_params = nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 17), (7, 41))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta1.shape, Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameters to be used in optimize.minimize\n",
    "\n",
    "#### *** Do not initialize parameters by yourself in this exam problem. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_nn_params = nn_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model representation\n",
    "\n",
    "This neural network has 3 layers - an input layer, a hidden layer and an output layer. \n",
    "\n",
    "The inputs are **16** features of the dry beans.\n",
    "\n",
    "The hidden layer has **40** neurons.\n",
    "\n",
    "The outputs are **7** dry bean types (0 to 6).\n",
    "\n",
    "The training data was loaded into the variables `X` and `y` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exam by yourself!\n",
    "input_layer_size  = 16\n",
    "hidden_layer_size = 40\n",
    "num_labels = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute sigmoid function given the input z.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        The input to the sigmoid function. This can be a 1-D vector \n",
    "        or a 2-D matrix. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    g : array_like\n",
    "        The computed sigmoid function. g has the same shape as z, since\n",
    "        the sigmoid is computed element-wise on z.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the sigmoid of each value of z (z can be a matrix, vector or scalar).\n",
    "    \"\"\"\n",
    "    # convert input to a numpy array\n",
    "    z = np.array(z)\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnCostFunction(nn_params, #weight\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels, #จำนวน output\n",
    "                   X, y, lambda_=1):\n",
    "    \"\"\"\n",
    "    Implements the neural network cost function and gradient for a one-hidden-layer neural \n",
    "    network which performs classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_params : array_like\n",
    "        The parameters for the neural network which are \"unrolled\" into \n",
    "        a vector. This needs to be converted back into the weight matrices Theta1\n",
    "        and Theta2.\n",
    "    \n",
    "    input_layer_size : int\n",
    "        Number of features for the input layer. \n",
    "    \n",
    "    hidden_layer_size : int\n",
    "        Number of hidden units in the second layer.\n",
    "    \n",
    "    num_labels : int\n",
    "        Total number of labels, or equivalently number of units in output layer. \n",
    "    \n",
    "    X : array_like\n",
    "        Input dataset. A matrix of shape (m x input_layer_size).\n",
    "    \n",
    "    y : array_like\n",
    "        Dataset labels. A vector of shape (m,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        Regularization parameter.\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed value for the cost function at the current weight values.\n",
    "        \n",
    "    grad : array_like\n",
    "        An \"unrolled\" vector of the partial derivatives of the concatenatation of\n",
    "        neural network weights Theta1 and Theta2.\n",
    "        \n",
    "    Instructions\n",
    "    ------------\n",
    "    You should complete the code by working through the following parts.\n",
    "    1) Section 1.3 Feedforward and cost function\n",
    "    2) Section 1.4 Regularized cost function\n",
    "    3) Section 2.3 Backpropagation\n",
    "    4) Section 2.5 Regularized Neural Network\n",
    "    \n",
    "    Complete each part after you finish reading that part.\n",
    "    \n",
    "    \"\"\"        \n",
    "\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our one-hidden-layer neural network\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))  # dimension = 25 X 401\n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                        (num_labels, (hidden_layer_size + 1)))  # dimension = 10 X 26\n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = []\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "\n",
    "    \n",
    "    # ====================== Code for Section 1.3 Feedforward and cost function ======================\n",
    "    \n",
    "    #Forward propagation\n",
    "    a1 = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "    z2 = np.dot(a1, Theta1.T) #dimension = 5000 x 25\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.concatenate([np.ones((m, 1)), a2], axis=1)  \n",
    "    z3 = np.dot(a2, Theta2.T)\n",
    "    h = sigmoid(z3) \n",
    "    \n",
    "    #Cost Function\n",
    "    y_matrix = np.eye(num_labels)[y]  #dimension = 3 x 10 #np.eye คือฟังก์ชัน identity matrix\n",
    "    J = (-1 / m) * np.sum(y_matrix * np.log(h) + (1 - y_matrix) * np.log(1 - h))\n",
    "    \n",
    "    # ====================== End of Code for Section 1.3 Feedforward and cost function ======================\n",
    "    #\n",
    "    #\n",
    "    # ====================== Code for Section 1.4 Regularized cost function ======================\n",
    "    # Add regularization term to the cost function from Section 1.3\n",
    "    # reg_term does not include the square of theta of bias units, \n",
    "    # For the matrices Theta1 and Theta2, this corresponds to the first column of each matrix.\n",
    "    # Therefore we use [:, 1:] to square every theta except the first column of the matrix (the theta of bias units)\n",
    "    \n",
    "    reg_term = (lambda_ / (2*m)) * (np.sum(np.square(Theta1[:,1:])) + np.sum(np.square(Theta2[:,1:])))\n",
    "    J += reg_term\n",
    "    \n",
    "    # ====================== End of Code for Section 1.4 Regularized cost function ======================\n",
    "    #\n",
    "    #\n",
    "    # ====================== Code for Section 2.3 Backpropagation ======================\n",
    "    delta_3 = h - y_matrix #dimension = 5000 x 10\n",
    "    \n",
    "    delta_2 = np.dot(delta_3,Theta2[:,1:]) * sigmoidGradient(z2) #dimension = 5000 x 25\n",
    "    \n",
    "    Delta1 = np.dot(delta_2.T,a1) #dimension = 25 x 401\n",
    "    Delta2 = np.dot(delta_3.T,a2) #dimension = 10 x 26\n",
    "    \n",
    "    Theta1_grad = (1/m) * Delta1 #dimension = 25 x 401\n",
    "    Theta2_grad = (1/m) * Delta2 #dimension = 10 x 26\n",
    "    \n",
    "    # To unroll the matrix into vector (1-D array), we use \"np.roll()\"\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "    \n",
    "    # ====================== End of Code for Section 2.3 Backpropagation ======================\n",
    "    #\n",
    "    #\n",
    "    # ====================== Code for Section 2.5 Regularized Neural Network ======================\n",
    "\n",
    "    Theta1_grad[1:] += (lambda_/m)*Theta1[1:]\n",
    "    Theta2_grad[1:] += (lambda_/m)*Theta2[1:]\n",
    "    \n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "    \n",
    "    \n",
    "    # ====================== End of Code for Section 2.5 Regularized Neural Network ======================\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.898535771545263"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.898583813225331"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sigmoid function evaluated at z. \n",
    "    This should work regardless if z is a matrix or a vector. \n",
    "    In particular, if z is a vector or matrix, you should return\n",
    "    the gradient for each element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A vector or matrix as input to the sigmoid function. \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    g : array_like\n",
    "        Gradient of the sigmoid function. Has the same shape as z. \n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the gradient of the sigmoid function evaluated at\n",
    "    each value of z (z can be a matrix, vector or scalar).    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    g = sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "    # =============================================================\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function utilsNN.predict(Theta1, Theta2, X)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 9.500000\n",
      "Training Set Accuracy: 10.400000\n"
     ]
    }
   ],
   "source": [
    "train = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(train == y) * 100))\n",
    "test = utils.predict(Theta1, Theta2, X_test)\n",
    "print('Training Set Accuracy: %f' % (np.mean(test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxfun to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxfun': 2000}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 1\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain the optimal Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3982737247519125"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_ = 1)\n",
    "debug_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 93.700000\n",
      "Training Set Accuracy: 93.333333\n"
     ]
    }
   ],
   "source": [
    "train = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(train == y) * 100))\n",
    "test = utils.predict(Theta1, Theta2, X_test)\n",
    "print('Training Set Accuracy: %f' % (np.mean(test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After you have completed the assignment, change the maxfun to a larger\n",
    "#  value to see how more training helps.\n",
    "options= {'maxfun': 2000}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 2\n",
    "\n",
    "# Create \"short hand\" for the cost function to be minimized\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "\n",
    "# Now, costFunction is a function that takes in only one argument\n",
    "# (the neural network parameters)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "# get the solution of the optimization\n",
    "nn_params = res.x\n",
    "        \n",
    "# Obtain the optimal Theta1 and Theta2 back from nn_params\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45100847716976517"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_ = 2)\n",
    "debug_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 93.416667\n",
      "Training Set Accuracy: 93.000000\n"
     ]
    }
   ],
   "source": [
    "train = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(train == y) * 100))\n",
    "test = utils.predict(Theta1, Theta2, X_test)\n",
    "print('Training Set Accuracy: %f' % (np.mean(test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Neural Network Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
