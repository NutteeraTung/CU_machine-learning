{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam Second Semester 2566 - Neural Network (Dry Bean Problem)\n",
    "\n",
    "This exam problem has an objective to develop a neural network model to classify the dry bean as belonging to one out of seven types (classes) from 16 features where\n",
    "\n",
    "type 0 = Seker,\n",
    "\n",
    "type 1 = Barbunya,\n",
    "\n",
    "type 2 = Bombay,\n",
    "\n",
    "type 3 = Cali,\n",
    "\n",
    "type 4 = Horoz,\n",
    "\n",
    "type 5 = Sira, and\n",
    "\n",
    "type 6 = Dermosan\n",
    "\n",
    "Cr: This dataset is adapted from KOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "import utils_NN as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We start the exam by first loading the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset and test dataset\n",
    "data = np.loadtxt(r'C:\\Users\\panda\\OneDrive\\Desktop\\fifaaa\\Data\\NN_BeanData_Train.txt')\n",
    "# First 16 columns of data are features and the last column is the label.\n",
    "# Matrix X contains 16 features while vector y contains the label.\n",
    "X, y = data[:, :16], data[:, 16].astype(int)\n",
    "m = y.size  # number of training examples\n",
    "\n",
    "# Read tab separated testing data\n",
    "data_test = np.loadtxt(r'C:\\Users\\panda\\OneDrive\\Desktop\\fifaaa\\Data\\NN_BeanData_Test.txt')\n",
    "X_test, y_test = data_test[:, :16], data_test[:, 16].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 16), (1500, 16))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have been provided with a set of initialized network parameters ($\\Theta^{(1)}, \\Theta^{(2)}$). These are stored in `InitBeanWeight1.txt` and `InitBeanWeight2.txt` which will be loaded in the next cell of this notebook into `Theta1` and `Theta2`. The parameters have dimensions that are sized for a neural network with 40 units in the second layer (hidden layer) and 7 output units (corresponding to 7 dry bean types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load initialized network parameters\n",
    "\n",
    "Theta1 = np.loadtxt(r'C:\\Users\\panda\\OneDrive\\Desktop\\fifaaa\\Data\\InitBeanWeight1.txt')\n",
    "Theta2 = np.loadtxt(r'C:\\Users\\panda\\OneDrive\\Desktop\\fifaaa\\Data\\InitBeanWeight2.txt')\n",
    "\n",
    "# Unroll parameters \n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 17), (7, 41))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta1.shape, Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial parameters to be used in optimize.minimize\n",
    "\n",
    "#### *** Do not initialize parameters by yourself in this exam problem. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_nn_params = nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_nn_params.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model representation\n",
    "\n",
    "This neural network has 3 layers - an input layer, a hidden layer and an output layer. \n",
    "\n",
    "The inputs are **16** features of the dry beans.\n",
    "\n",
    "The hidden layer has **40** neurons.\n",
    "\n",
    "The outputs are **7** dry bean types (0 to 6).\n",
    "\n",
    "The training data was loaded into the variables `X` and `y` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters you will use for this exam by yourself!\n",
    "input_layer_size  = 16 #features\n",
    "hidden_layer_size = 40 #hidden units\n",
    "num_labels = 7 #labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Feed Forward (Forward Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = np.array(z)\n",
    "    g = np.zeros(z.shape)\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g( 0 ) =  0.5\n"
     ]
    }
   ],
   "source": [
    "# Test the implementation of sigmoid function here\n",
    "z = 0\n",
    "g = sigmoid(z)\n",
    "\n",
    "print('g(',z,') = ', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost function of neural network\n",
    "def nnCostFunction(nn_params,\n",
    "                   input_layer_size,\n",
    "                   hidden_layer_size,\n",
    "                   num_labels,\n",
    "                   X, y, lambda_=0.0):\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                        (hidden_layer_size, (input_layer_size + 1)))  \n",
    "\n",
    "    Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):], \n",
    "                        (num_labels, (hidden_layer_size + 1)))  \n",
    "\n",
    "    # Setup some useful variables\n",
    "    m = y.size\n",
    "         \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = []\n",
    "    Theta1_grad = np.zeros(Theta1.shape) \n",
    "    Theta2_grad = np.zeros(Theta2.shape) \n",
    "\n",
    "    \"\"\" Feed Forward \"\"\"\n",
    "    #let a1 = X and add the bias unit\n",
    "    a1 = np.concatenate([np.ones((m,1)),X],axis = 1) \n",
    "    z2 = np.dot(a1,Theta1.T) \n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.concatenate([np.ones((a2.shape[0], 1)), a2],axis=1) \n",
    "    z3 = np.dot(a2,Theta2.T) \n",
    "    a3 = sigmoid(z3)\n",
    "    # h = activation unit values of the last output layer\n",
    "    h = a3 \n",
    "\n",
    "    \"\"\" Cost Function\"\"\"\n",
    "    y_matrix = y\n",
    "    y_matrix = np.eye(num_labels)[y_matrix] #dimen = 5000 x 10\n",
    "    logprobs = np.multiply(np.log(h),y_matrix) + np.multiply(np.log(1-h),1-y_matrix)\n",
    "    J = (-1/m)*np.sum(logprobs)\n",
    "\n",
    "    \"\"\"Regularization term\"\"\"\n",
    "    reg_term = (lambda_ / (2 * m)) * (np.sum(np.square(Theta1[:,1:])) \\\n",
    "                                        + np.sum(np.square(Theta2[:,1:])))\n",
    "    J = J + reg_term\n",
    "    \n",
    "    \"\"\"Back Propagation\"\"\"\n",
    "    delta_3 = h - y_matrix #dimen = 5000 x 10\n",
    "    delta_2 = np.dot(delta_3, Theta2[:,1:]) * sigmoidGradient(z2) #dimen = 5000 x 25\n",
    "    Delta1 = np.dot(delta_2.T, a1) #dimen = 25 x 401\n",
    "    Delta2 = np.dot(delta_3.T, a2) #dimen = 10 x 26\n",
    "    Theta1_grad = (1/m) * Delta1\n",
    "    Theta2_grad = (1/m) * Delta2\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "    \n",
    "    \"\"\"Regularized to gradient\"\"\"\n",
    "    Theta1_grad[:,1:] = Theta1_grad[:,1:] + (lambda_ /m) * Theta1[:,1:]\n",
    "    Theta2_grad[:,1:] = Theta2_grad[:,1:] + (lambda_ /m) * Theta2[:,1:]\n",
    "    \n",
    "    #update grad with regularization\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad),np.ravel(Theta2_grad)])\n",
    "    \n",
    "    return J, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from NN_weights): 4.898536 \n"
     ]
    }
   ],
   "source": [
    "#run sigmoid gradient first\n",
    "lambda_ = 0\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "print('Cost at parameters (loaded from NN_weights): %.6f ' % J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at parameters (loaded from NN_weights): 4.898584 \n"
     ]
    }
   ],
   "source": [
    "#run sigmoid gradient first\n",
    "#ไม่ regularized bias\n",
    "lambda_ = 1\n",
    "J, _ = nnCostFunction(nn_params, input_layer_size, hidden_layer_size,\n",
    "                   num_labels, X, y, lambda_)\n",
    "print('Cost at parameters (loaded from NN_weights): %.6f ' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidGradient(z):\n",
    "    g = np.zeros(z.shape)\n",
    "    g = sigmoid(z)*(1-sigmoid(z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\n",
      "  \n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-1, -0.5, 0, 0.5, 1])\n",
    "g = sigmoidGradient(z)\n",
    "print('Sigmoid gradient evaluated at [-1 -0.5 0 0.5 1]:\\n  ')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function utils_NN.predict(Theta1, Theta2, X)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-3.04978709e-06 -3.04978914e-06]\n",
      " [-1.75060084e-04 -1.75060082e-04]\n",
      " [-9.62660640e-05 -9.62660620e-05]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 1.42869450e-05  1.42869443e-05]\n",
      " [ 2.33146358e-04  2.33146357e-04]\n",
      " [ 1.17982666e-04  1.17982666e-04]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [-2.59383093e-05 -2.59383100e-05]\n",
      " [-2.87468729e-04 -2.87468729e-04]\n",
      " [-1.37149709e-04 -1.37149706e-04]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 3.69883257e-05  3.69883234e-05]\n",
      " [ 3.35320351e-04  3.35320347e-04]\n",
      " [ 1.53247082e-04  1.53247082e-04]\n",
      " [-6.74798369e-03 -6.74798370e-03]\n",
      " [-4.68759742e-05 -4.68759769e-05]\n",
      " [-3.76215583e-04 -3.76215587e-04]\n",
      " [-1.66560294e-04 -1.66560294e-04]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.64090819e-01  1.64090819e-01]\n",
      " [ 1.64567932e-01  1.64567932e-01]\n",
      " [ 1.58339334e-01  1.58339334e-01]\n",
      " [ 1.51127527e-01  1.51127527e-01]\n",
      " [ 1.49568335e-01  1.49568335e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 5.75736494e-02  5.75736493e-02]\n",
      " [ 5.77867378e-02  5.77867378e-02]\n",
      " [ 5.59235296e-02  5.59235296e-02]\n",
      " [ 5.36967009e-02  5.36967009e-02]\n",
      " [ 5.31542052e-02  5.31542052e-02]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 5.04575855e-02  5.04575855e-02]\n",
      " [ 5.07530173e-02  5.07530173e-02]\n",
      " [ 4.91620841e-02  4.91620841e-02]\n",
      " [ 4.71456249e-02  4.71456249e-02]\n",
      " [ 4.65597186e-02  4.65597186e-02]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.34726e-11\n"
     ]
    }
   ],
   "source": [
    "#Gradient Checking\n",
    "utils.checkNNGradients(nnCostFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.27825235e-03 -9.27825236e-03]\n",
      " [-1.67679797e-02 -1.67679797e-02]\n",
      " [-6.01744725e-02 -6.01744725e-02]\n",
      " [-1.73704651e-02 -1.73704651e-02]\n",
      " [ 8.89911959e-03  8.89911960e-03]\n",
      " [ 3.94334829e-02  3.94334829e-02]\n",
      " [-3.19612287e-02 -3.19612287e-02]\n",
      " [-5.75658668e-02 -5.75658668e-02]\n",
      " [-8.36010761e-03 -8.36010762e-03]\n",
      " [ 5.93355565e-02  5.93355565e-02]\n",
      " [ 2.49225535e-02  2.49225535e-02]\n",
      " [-4.51963845e-02 -4.51963845e-02]\n",
      " [ 7.62813550e-03  7.62813551e-03]\n",
      " [ 2.47640974e-02  2.47640974e-02]\n",
      " [ 5.97717617e-02  5.97717617e-02]\n",
      " [ 9.14587966e-03  9.14587966e-03]\n",
      " [-6.74798369e-03 -6.74798370e-03]\n",
      " [-3.26881426e-02 -3.26881426e-02]\n",
      " [ 3.86410548e-02  3.86410548e-02]\n",
      " [ 5.46101547e-02  5.46101547e-02]\n",
      " [ 3.14544970e-01  3.14544970e-01]\n",
      " [ 1.18682669e-01  1.18682669e-01]\n",
      " [ 2.03987128e-01  2.03987128e-01]\n",
      " [ 1.25698067e-01  1.25698067e-01]\n",
      " [ 1.76337550e-01  1.76337550e-01]\n",
      " [ 1.32294136e-01  1.32294136e-01]\n",
      " [ 1.11056588e-01  1.11056588e-01]\n",
      " [ 3.81928689e-05  3.81928696e-05]\n",
      " [ 1.17148233e-01  1.17148233e-01]\n",
      " [-4.07588279e-03 -4.07588279e-03]\n",
      " [ 1.13133142e-01  1.13133142e-01]\n",
      " [-4.52964427e-03 -4.52964427e-03]\n",
      " [ 9.74006970e-02  9.74006970e-02]\n",
      " [ 3.36926556e-02  3.36926556e-02]\n",
      " [ 7.54801264e-02  7.54801264e-02]\n",
      " [ 1.69677090e-02  1.69677090e-02]\n",
      " [ 8.61628953e-02  8.61628953e-02]\n",
      " [ 1.50048382e-03  1.50048382e-03]]\n",
      "The above two columns you get should be very similar.\n",
      "(Left-Your Numerical Gradient, Right-Analytical Gradient)\n",
      "\n",
      "If your backpropagation implementation is correct, then \n",
      "the relative difference will be small (less than 1e-9). \n",
      "Relative Difference: 2.23546e-11\n",
      "\n",
      "\n",
      "Cost at (fixed) debugging parameters (w/ lambda = 3.000000): 4.898680 \n"
     ]
    }
   ],
   "source": [
    "#  Check gradients by running checkNNGradients\n",
    "lambda_ = 3\n",
    "utils.checkNNGradients(nnCostFunction, lambda_)\n",
    "\n",
    "# Also output the costFunction debugging values\n",
    "debug_J, _  = nnCostFunction(nn_params, input_layer_size,\n",
    "                          hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "print('\\n\\nCost at (fixed) debugging parameters (w/ lambda = %f): %f ' % (lambda_, debug_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not random weight and make a function of randInitializeWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panda\\AppData\\Local\\Temp\\ipykernel_6220\\3202094118.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs = np.multiply(np.log(h),y_matrix) + np.multiply(np.log(1-h),1-y_matrix)\n",
      "C:\\Users\\panda\\AppData\\Local\\Temp\\ipykernel_6220\\3202094118.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs = np.multiply(np.log(h),y_matrix) + np.multiply(np.log(1-h),1-y_matrix)\n"
     ]
    }
   ],
   "source": [
    "options= {'maxfun':500} #adjust maxfun\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 0.11\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "\n",
    "nn_params = res.x\n",
    "        \n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function when lambda =  0.11 is  0.3593126217340986\n"
     ]
    }
   ],
   "source": [
    "print('Cost function when lambda = ', lambda_,'is ', res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_J, _  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_ = 1)\n",
    "# debug_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 93.500000\n",
      "Training Set Accuracy: 93.133333\n"
     ]
    }
   ],
   "source": [
    "pred_train = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_train == y) * 100))\n",
    "pred_test = utils.predict(Theta1, Theta2, X_test)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "options= {'maxfun': 2000}\n",
    "\n",
    "#  You should also try different values of lambda\n",
    "lambda_ = 2\n",
    "costFunction = lambda p: nnCostFunction(p, input_layer_size,\n",
    "                                        hidden_layer_size,\n",
    "                                        num_labels, X, y, lambda_)\n",
    "res = optimize.minimize(costFunction,\n",
    "                        initial_nn_params,\n",
    "                        jac=True,\n",
    "                        method='TNC',\n",
    "                        options=options)\n",
    "nn_params = res.x\n",
    "Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)],\n",
    "                    (hidden_layer_size, (input_layer_size + 1)))\n",
    "\n",
    "Theta2 = np.reshape(nn_params[(hidden_layer_size * (input_layer_size + 1)):],\n",
    "                    (num_labels, (hidden_layer_size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function when lambda =  2 is  0.4189075576751514\n"
     ]
    }
   ],
   "source": [
    "print('Cost function when lambda = ', lambda_,'is ', res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_J, _  = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_ = 2)\n",
    "# debug_J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy: 93.650000\n",
      "Training Set Accuracy: 93.400000\n"
     ]
    }
   ],
   "source": [
    "pred_train = utils.predict(Theta1, Theta2, X)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_train == y) * 100))\n",
    "pred_test = utils.predict(Theta1, Theta2, X_test)\n",
    "print('Training Set Accuracy: %f' % (np.mean(pred_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.displayData(Theta1[:, 1:])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Neural Network Problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
